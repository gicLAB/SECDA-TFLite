{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 11:09:09.338926: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2025-02-20 11:09:09.338946: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2025-02-20 11:09:09.339038: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: saved_model\n",
      "2025-02-20 11:09:09.339234: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2025-02-20 11:09:09.339248: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: saved_model\n",
      "2025-02-20 11:09:09.339685: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2025-02-20 11:09:09.346363: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: saved_model\n",
      "2025-02-20 11:09:09.348376: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 9338 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define a simple model with an Add layer\n",
    "class SimpleAddModel(tf.keras.Model):\n",
    "  def __init__(self, dim_size):\n",
    "    super(SimpleAddModel, self).__init__()\n",
    "    self.dim_size = dim_size\n",
    "    self.add = tf.keras.layers.Add()\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return self.add(inputs)\n",
    "\n",
    "\n",
    "# Define the dimension size\n",
    "dim_size = 5\n",
    "\n",
    "# Create an instance of the model\n",
    "model = SimpleAddModel(dim_size)\n",
    "\n",
    "# Define some dummy input data\n",
    "input_data = [np.random.rand(1, dim_size).astype(np.float32), np.random.rand(1, dim_size).astype(np.float32)]\n",
    "\n",
    "# Build the model by calling it\n",
    "model(input_data)\n",
    "\n",
    "# Save the model to a SavedModel format\n",
    "saved_model_dir = \"saved_model\"\n",
    "tf.saved_model.save(model, saved_model_dir)\n",
    "\n",
    "# Convert the model to TFLite with INT8 quantization\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "def representative_dataset_gen():\n",
    "  for _ in range(100):\n",
    "    yield [np.random.rand(1, dim_size).astype(np.float32), np.random.rand(1, dim_size).astype(np.float32)]\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "file_name = \"models/add/model_int8.tflite\"\n",
    "\n",
    "# Save the TFLite model to a file\n",
    "with open(file_name, \"wb\") as f:\n",
    "  f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
